{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2-Laboratory-15-10-2020**\n",
    "\n",
    "| Credits to the authors of the exercises: Andrea Pasini, Giuseppe Attanasio, Flavio Giobergia <br />\n",
    "| Master of Science in Data Science and Engineering, Politecnico di Torino, A.A. 2020-21\n",
    "\n",
    "## Global Land Temperature - Data preprocessing\n",
    "The Global Land Temperature (GLT) dataset is a large collection of measurements actively maintained by Berkeley Earth. It contains the raw source data measured with stations all around the globe, plus anintermediate format and several formatted output files. Data span from ∼1750 up to recent days with monthly and daily availability. \n",
    "<br />\n",
    "Measurements are provided by hemispheres, states, countries, cities andmore. You can read more about the dataset at the Berkeley Earth website. For the purpose of this laboratory you will work on a modified, smaller but dirtier, version of the original GLT dataset, to stress the importance of data preprocessing. More specifically, this didactic version contains the formatted output files of the major cities of the globe with monthly granularity. For the sake of simplicity, the analysis will range between almost two centuries (i.e. between the years 1817 and 2012). The dataset is composed of∼200k rows corresponding to the measurements taken the first day of themonth in a given city. Each measurement is then described by 7 values:\n",
    "- Date, when the measurement was taken\n",
    "- AverageTemperature\n",
    "- AverageTemperatureUncertainty\n",
    "- City, from which the measurement was taken•\n",
    "- Country\n",
    "- Latitude\n",
    "- Longitude\n",
    "\n",
    "The main goal of this exercise is to learn how to clean a real-world dataset searching for anomalies, suchas missing values or outliers, in its data\n",
    "\n",
    "### Questions\n",
    "1. Load the Global Land Temperature dataset as a list of lists. Before starting, take a moment to better inspect the attributes you are going to work on. How many of them are nominal, how many continuous or discrete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# initialize\n",
    "GLT_dataset = [ [] for i in range(7)]\n",
    "\n",
    "with open('../Datasets/GLT_filtered.csv') as f:\n",
    "    \n",
    "    for row in csv.reader(f): \n",
    "        for i in range(len(row)):\n",
    "            GLT_dataset[i].append(row[i])\n",
    "        \n",
    "head = [ GLT_dataset[i].pop(0) for i in range(len(GLT_dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see better what we obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** Head  \n",
      "['Date', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City', 'Country', 'Latitude', 'Longitude']\n",
      "\n",
      " *** First 5 elements for each list\n",
      "['1849-01-01', '1849-02-01', '1849-03-01', '1849-04-01', '1849-05-01', '1849-06-01', '1849-07-01', '1849-08-01']\n",
      "['26.704', '27.434', '', '26.14', '25.427', '24.844', '24.058000000000003', '23.576']\n",
      "['1.435', '1.3619999999999999', '', '1.3869999999999998', '1.2', '1.402', '1.254', '1.265']\n",
      "['Abidjan', 'Abidjan', 'Abidjan', 'Abidjan', 'Abidjan', 'Abidjan', 'Abidjan', 'Abidjan']\n",
      "[\"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\", \"CÃ´te D'Ivoire\"]\n",
      "['5.63N', '5.63N', '5.63N', '5.63N', '5.63N', '5.63N', '5.63N', '5.63N']\n",
      "['3.23W', '3.23W', '3.23W', '3.23W', '3.23W', '3.23W', '3.23W', '3.23W']\n"
     ]
    }
   ],
   "source": [
    "print(f\" *** Head  \\n{head}\")\n",
    "print(\"\\n *** First 5 elements for each list\")\n",
    "\n",
    "for i in range(len(GLT_dataset)):\n",
    "    print(GLT_dataset[i][:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we're dealing with 2 nominal attributes (City,Country), three discrete ones (Date,Latitude,Longitude) and finally two continuos attributes (AverageTemperature,AverageTemperatureUncertainty) <br />\n",
    "\n",
    "2. Analyze the attribute AverageTemperature, which contains missing values. Fill any gap with the arithmetic mean among the closest antecedent and the closest successive measurements in time,taken in the same city. Assume the following rules for edge cases:\n",
    "\n",
    "original_list = ['', 5, 6,'']\n",
    "step_1        = [ 2.5, 5, 6,''] # (0 + 5) / 2\n",
    "step_2        = [ 2.5, 5, 6,  3 ] # (6 + 0) / 2\n",
    "\n",
    "original_list   = ['','', 24, 28.9 ]\n",
    "step_1          = [ 12,'', 24, 28.9 ] # (0 + 24) / 2\n",
    "step_2          = [ 12, 18, 24, 28.9 ] # (12 + 24) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 5, 4, '', 3, '', '']\n",
      "[2.5, 5, 4, '', 3, '', '']\n",
      "[2.5, 5, 4, '', 3, '', '']\n",
      "[2.5, 5, 4, '', 3, '', '']\n",
      "[2.5, 5, 4, 3.5, 3, '', '']\n",
      "[2.5, 5, 4, 3.5, 3, '', '']\n",
      "[2.5, 5, 4, 3.5, 3, 1.5, '']\n",
      "[2.5, 5, 4, 3.5, 3, 1.5, 0.75]\n"
     ]
    }
   ],
   "source": [
    "# let's start with a toy vector in order to see better what happens\n",
    "toy =  ['',5,4,'',3,'','']\n",
    "\n",
    "def next_non_negative(i):\n",
    "    for k in range(i+1,len(toy)):\n",
    "        if toy[k] != '':\n",
    "            return k\n",
    "    return -1\n",
    "\n",
    "\"\"\" \n",
    "    I define the first control outside the loop \n",
    "    because in this way this control will take place\n",
    "    just once\n",
    "\"\"\"\n",
    "print(toy)\n",
    "\n",
    "if toy[0] == '':\n",
    "    toy[0] = toy[next_non_negative(0)]/2\n",
    "\n",
    "print(toy)\n",
    "\n",
    "# from second to penultimate\n",
    "for i in range(1,len(toy)):\n",
    "    \n",
    "    if toy[i] == '':\n",
    "        j = next_non_negative(i)\n",
    "        \n",
    "        # if there is any non-null value beyond it, it takes the previous one\n",
    "        if j == -1:\n",
    "            toy[i] = toy[i-1]/2\n",
    "        else:\n",
    "            if (j-i) > 1:\n",
    "                toy[i] = toy[j]/2\n",
    "            else:\n",
    "                toy[i] = (toy[i-1] + toy[j])/2\n",
    "    print(toy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'd works, now let's try it on our list, but firstly we need to map these values as float, otherwise we cannot calculate the mean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.704,\n",
       " 27.434,\n",
       " 26.787,\n",
       " 26.14,\n",
       " 25.427,\n",
       " 24.844,\n",
       " 24.058000000000003,\n",
       " 23.576,\n",
       " 24.4195,\n",
       " 25.263]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from string to float\n",
    "GLT_dataset[1] = [float(i) if i != '' else '' for i in GLT_dataset[1] ]\n",
    "\n",
    "def next_non_negativeV2(i):\n",
    "    for k in range(i+1,len(GLT_dataset[1])):\n",
    "        if GLT_dataset[1][k] != '':\n",
    "            return k\n",
    "    return -1\n",
    "\n",
    "if GLT_dataset[1][0] == '':\n",
    "    GLT_dataset[1][0] = GLT_dataset[1][next_non_negative(0)]/2\n",
    "\n",
    "for i in range(len(GLT_dataset[1])):\n",
    "    \n",
    "    if GLT_dataset[1][i] == '':\n",
    "        \n",
    "        j = next_non_negativeV2(i)\n",
    "        \n",
    "        if j == -1:\n",
    "            GLT_dataset[1][i] = GLT_dataset[1][i-1]/2\n",
    "        else:\n",
    "            if (j-i) > 1:\n",
    "                GLT_dataset[1][i] = GLT_dataset[1][j]/2\n",
    "            else:\n",
    "                GLT_dataset[1][i] = (GLT_dataset[1][i-1] + GLT_dataset[1][j])/2\n",
    "    \n",
    "GLT_dataset[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.704,\n",
       " 27.434,\n",
       " 13.07,\n",
       " 26.14,\n",
       " 25.427,\n",
       " 24.844,\n",
       " 24.058000000000003,\n",
       " 23.576,\n",
       " 12.6315,\n",
       " 25.263]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,elem in enumerate(GLT_dataset[1]):\n",
    "    \n",
    "    if elem == '' and i != len(GLT_dataset[1])-1:\n",
    "        # look for the first non-null value\n",
    "        for j in range(i+1,len(GLT_dataset[1])):\n",
    "            if GLT_dataset[1][j] != '':\n",
    "                GLT_dataset[1][i] = GLT_dataset[1][j]/2\n",
    "                break\n",
    "                \n",
    "    elif elem == '' and i == len(GLT_dataset[1])-1:\n",
    "        # use the previous element\n",
    "        GLT_dataset[1][i] = GLT_dataset[1][i-1]/2 \n",
    "\n",
    "GLT_dataset[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a function that, given the name of a city and an integerN >0, prints:<br />\n",
    "(a) the top N hottest measurements; <br />\n",
    "(b) the top N coldest measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hottest *** \n",
      "[29.923, 29.596, 29.477, 29.471]\n",
      "\n",
      "Coldest *** \n",
      "[11.409500000000003, 12.001, 12.163499999999999, 12.163499999999999, 12.2215]\n"
     ]
    }
   ],
   "source": [
    "def getTopTemperatures(city,N):\n",
    "    # array with all the temperatures of records with the given city\n",
    "    temp_city = [ GLT_dataset[1][i] for i,elem in enumerate(GLT_dataset[3]) if elem == city]\n",
    "    temp_city.sort()\n",
    "    \n",
    "    #      coldest         hottest\n",
    "    return [temp_city[:N],temp_city[:-N:-1]]\n",
    "\n",
    "coldest, hottest = getTopTemperatures(\"Abidjan\",5)\n",
    "\n",
    "print(f\"Hottest *** \\n{hottest}\\n\")\n",
    "print(f\"Coldest *** \\n{coldest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) 4. Let’s search for other anomalies in data distribution with the help of matplotlib. Plot the distribution of the average land temperatures for Rome and Bangkok using the aforementioned histogram plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l = [record[1] for record in GLT_dataset if record[3]==\"Rome\"] \n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-022ab8e03d5e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-39-022ab8e03d5e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for record in GLT_dataset:\n",
    "    print(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) One might think that Bangkok sensor provide temperature samples in degrees Fahrenheit while the ones located in Rome use the Celsius notation, which is the common representation in the whole dataset. Write a function to transform Fahrenheit measurements back to Celsius, apply it to your data and plot the two distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb reviews - Textual data preparation \n",
    "This exercise is meant to get you acquainted with the preprocessing of textual data. You can find useful information about Information Retrieval theory in the book \"Introduction to Information Retrieval\" (Manning, Raghavan, and Schütze 2008) also available at Introduction to Information Retrieval Stanford webpage. <br />\n",
    "\n",
    "Internet Movie Database (IMDb) is a popular online platform that gathers many information related tomultimed like movies, tv shows, video games and many more. You can navigate to its website to explore the huge amount of updated content it offers. The only difference with the previously used CSV datasetsis that, in this one, the first row of the file contains a “header” (i.e. the name of each column). You should skip the first row and start reading from the second one on. <br />\n",
    "IMDb has soon become a rich data source for the scientific community. Among the others, people’s reviews are one of the most important types of data that can be retrieved from the platform. The collection of reviews on a specific movie, for example, contains intrinsic information about its approval rating. Therefore, a considerable amount of recent science works has addressed the identification of the sentimentwithin textual reviews and surveys. In the context of movie reviews, the sentiment analysis would seek todiscover if the reviewer liked the movie or not based on the content of the text, with the sentiment being represented as numerical value (e.g. a score between 1 and 10) or a binary one (e.gPositiveorNegative). Wikipedia provides a general overview on the topic of sentiment analysis.During this laboratory you will work on the dataset collected and used by Maas et al. 2011. We will focus on a smaller portion of the whole dataset, known as the training data (you will learn more ontraining and test datasets). It contains 25,000 user reviews collected from IMDb for different movies.Since each review has also a numerical score between 1 and 10, the authors considered the ones witha score lower than 4 to have a negative sentiment, while the ones with a score higher than 6 to have apositive sentiment. The dataset includes12,500positive and12,500negative reviews. For the purpose ofthis laboratory, reviews, which originally came in different files, have been organized in a single CSV file.Each line of the file refers to a single review and has two fields:\n",
    "1. the textual comment\n",
    "2. a binary value indicating either the positive or negative sentiment, represented respectively by a ’1’and a ’0’.\n",
    "\n",
    "### Questions\n",
    "1. Load the IMDb dataset as a list of lists.\n",
    "\n",
    "All the rows have the same structure, the comment starts and ends with a \", and it is followed by the relative number. So we can use that structure for the extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"\"The Skipper\"\" Hale jr. as a police Sgt.', \"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", 'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"\"Flamingo Road\"\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"\"Rosemary\\'s Baby\"\" and \"\"The Exorcist\"\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"\"The Sentinel\"\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****'] \n",
      "\n",
      " ['1', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# Using readlines() \n",
    "IMDb_dataset = [ [], [] ]\n",
    "head = [\"review\",\"label\"]\n",
    "\n",
    "with open('../Datasets/aclimdb_reviews_train.txt',encoding=\"utf8\") as f:\n",
    "    # skip 1st line (head)\n",
    "    next(f)\n",
    "    \n",
    "    for line in f:\n",
    "        \"\"\"\n",
    "            the rows have always the same structure\n",
    "            \" -- comment -- \",0/1 \\n\n",
    "            with [1:-3] I get just the comment, without the \"\"\n",
    "            with [-1] I get just the last element (number)\n",
    "        \"\"\"\n",
    "        IMDb_dataset[0].append((line.rstrip('\\n')[1:-3]))\n",
    "        IMDb_dataset[1].append((line.rstrip('\\n')[-1]))\n",
    "\n",
    "print(IMDb_dataset[0][:3],\"\\n\\n\",IMDb_dataset[1][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply the tokenization function listed below to your reviews. Please refer to the function’s docstring 1 for the input and output parameters. The tokenization procedure splits each comment in tokens (i.e.separate words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# written by the instructors\n",
    "def tokenize(docs):\n",
    "    \"\"\"\n",
    "        Compute the tokens for each document.\n",
    "        Input: a list of strings. Each item is a document to tokenize.\n",
    "        Output: a list of lists. Each item is a list containing the tokens of therelative document.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        for punct in string.punctuation:\n",
    "            doc = doc.replace(punct, \" \")\n",
    "        \n",
    "        split_doc = [ token.lower() for token in doc.split(\" \") if token ]\n",
    "        tokens.append(split_doc)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr', 'as', 'a', 'police', 'sgt'] ['working', 'with', 'one', 'of', 'the', 'best', 'shakespeare', 'sources', 'this', 'film', 'manages', 'to', 'be', 'creditable', 'to', 'it', 's', 'source', 'whilst', 'still', 'appealing', 'to', 'a', 'wider', 'audience', 'br', 'br', 'branagh', 'steals', 'the', 'film', 'from', 'under', 'fishburne', 's', 'nose', 'and', 'there', 's', 'a', 'talented', 'cast', 'on', 'good', 'form'] "
     ]
    }
   ],
   "source": [
    "tokenList = tokenize(IMDb_dataset[0])\n",
    "\n",
    "for i in tokenList[:2]:\n",
    "    print(i, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The next step requires the computation of the term frequency (TF) of each token within its respective document. Although there exist different techniques to evaluate the frequency, we will now assumethat the TF of a tokentin a documentdis equal to the number of occurrences oftind. Computethe TF for all your reviews.\n",
    "\n",
    "In this case we have to create a dictionary and iterate each word in each comment. If the word is already in the dictionary we need to increase its value by 1, otherwise we initialize its value to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each TF will refeer to just one comment, so we need a list of dict\n",
    "term_frequency = []\n",
    "\n",
    "for comment in tokenList:\n",
    "    term_frequency_dic = {}\n",
    "    \n",
    "    for word in comment:\n",
    "        if word in term_frequency_dic:\n",
    "            term_frequency_dic[word] += 1\n",
    "        else:\n",
    "            term_frequency_dic[word] = 1\n",
    "            \n",
    "    term_frequency.append(term_frequency_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'for': 3,\n",
       " 'a': 5,\n",
       " 'movie': 2,\n",
       " 'that': 1,\n",
       " 'gets': 1,\n",
       " 'no': 1,\n",
       " 'respect': 1,\n",
       " 'there': 1,\n",
       " 'sure': 1,\n",
       " 'are': 1,\n",
       " 'lot': 1,\n",
       " 'of': 1,\n",
       " 'memorable': 1,\n",
       " 'quotes': 1,\n",
       " 'listed': 1,\n",
       " 'this': 1,\n",
       " 'gem': 1,\n",
       " 'imagine': 1,\n",
       " 'where': 1,\n",
       " 'joe': 1,\n",
       " 'piscopo': 1,\n",
       " 'is': 3,\n",
       " 'actually': 1,\n",
       " 'funny': 1,\n",
       " 'maureen': 1,\n",
       " 'stapleton': 1,\n",
       " 'scene': 1,\n",
       " 'stealer': 1,\n",
       " 'the': 2,\n",
       " 'moroni': 1,\n",
       " 'character': 1,\n",
       " 'an': 1,\n",
       " 'absolute': 1,\n",
       " 'scream': 1,\n",
       " 'watch': 1,\n",
       " 'alan': 1,\n",
       " 'skipper': 1,\n",
       " 'hale': 1,\n",
       " 'jr': 1,\n",
       " 'as': 1,\n",
       " 'police': 1,\n",
       " 'sgt': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try printing just few elements\n",
    "term_frequency[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We will now compute the inverse document frequency(IDF). While the TF gives an idea of the weight of a token within a document, the IDF is used to find its significance among the entire collection ofdocuments (i.e. your reviews). One possible way of computing it is:\n",
    "\n",
    "    IDF_t = log (N/DF_t)\n",
    "\n",
    "    Where N is the number of documents and DFt is the document frequency of a token, i.e. the number of documents in whicht appears at least once. As you can see, IDFt ∈ [0,logN]. Furthermore, a low value means that the token appears in the majority of the documents, hence its presence isnot relevant to characterize any subset of them, whereas an high value indicates relevance for a fewdocuments.\n",
    "\n",
    "    (a) Compute the DF for all of your tokens;<br />\n",
    "    (b) Compute the IDF for all of your tokens;<br />\n",
    "    (c) Try to sort the IDF values in ascending order. Which tokens (i.e. words) came to the top? Can you figure out why?\n",
    "    \n",
    "I will use the previous dictionary in order to select the tokens just once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.008314469604085238),\n",
       " ('a', 0.03355678352884277),\n",
       " ('and', 0.03401190259170586),\n",
       " ('of', 0.05226218466281087),\n",
       " ('to', 0.06293979977387414),\n",
       " ('this', 0.10136849438828884),\n",
       " ('is', 0.1086102347240488),\n",
       " ('it', 0.11559044114948828),\n",
       " ('in', 0.12606221366364628),\n",
       " ('that', 0.20722099077039452)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import operator #efficient order\n",
    "\n",
    "document_frequency = {}\n",
    "N = len(term_frequency)\n",
    "\n",
    "for comment in term_frequency:\n",
    "    for elem in comment:\n",
    "        if elem in document_frequency:\n",
    "            document_frequency[elem] += 1\n",
    "        else:\n",
    "            document_frequency[elem] = 1\n",
    "\n",
    "inverse_document_frequency = {elem: math.log(N/document_frequency[elem]) for elem in document_frequency}\n",
    "IDF_sorted = sorted(inverse_document_frequency.items(), key=operator.itemgetter(1))\n",
    "IDF_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first elements are the most frequent among the document. Of course I expected this results, in fact articles, prepositions and other similar words concern a \"short range\" of term and they're frequently used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compute the TF-IDF. Combine the definitions of term frequency (TF) and inverse document fre-quency (IDF), to produce a composite weight for each term in each document. The TF-IDF weighting scheme assigns to a termta weight in the documentdgiven by:\n",
    "\n",
    "    TF-IDF_t,d  = TF_t,d * IDF_t\n",
    "    \n",
    "    In other words, TF-IDFt,d assigns to term t a weight in document d that is\n",
    "    - high whentoccurs many times within a small number of documents;\n",
    "    - low when the term occurs fewer times in a document, or occurs in many documents (thusoffering a less pronounced relevance signal);\n",
    "    - lowest when the term occurs in virtually all documents.\n",
    "    \n",
    "  For dictionary terms that do not occur in a document, the weight is zero.The suggested output structure is a list of dictionaries. Each dictionary represents a document andcontains its tokens as key and weights as values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for': 1.0054113848254047, 'a': 0.16778391764421385, 'movie': 0.981523185460498, 'that': 0.20722099077039452, 'gets': 2.257229391273248, 'no': 1.1142539846580666, 'respect': 3.9845936982629815, 'there': 0.8380342339083877, 'sure': 2.3538783873815965, 'are': 0.5869150448732452, 'lot': 2.0319474551515233, 'of': 0.05226218466281087, 'memorable': 3.6936910111111585, 'quotes': 5.5940316106970815, 'listed': 5.339139361068292, 'this': 0.10136849438828884, 'gem': 4.291820366787733, 'imagine': 3.5884912800826676, 'where': 1.655900786844441, 'joe': 4.137669686960474, 'piscopo': 7.418580902748128, 'is': 0.3258307041721464, 'actually': 1.982532640511814, 'funny': 2.074653024948039, 'maureen': 6.437751649736401, 'stapleton': 7.561681746388801, 'scene': 1.8767946184246356, 'stealer': 7.487573774235079, 'the': 0.016628939208170476, 'moroni': 8.740336742730447, 'character': 1.641547966352334, 'an': 0.7165386400173159, 'absolute': 4.315490110873637, 'scream': 4.706096104578052, 'watch': 1.5199629060064976, 'alan': 4.625372893305611, 'skipper': 7.929406526514119, 'hale': 6.515713191206113, 'jr': 4.5932416151228175, 'as': 0.4391253096007996, 'police': 3.460947386067929, 'sgt': 6.4630694577206915}\n"
     ]
    }
   ],
   "source": [
    "tf_idf = []\n",
    "\n",
    "for i,comment in enumerate(term_frequency):\n",
    "    tf_idf_dic = {}\n",
    "    for elem in comment:\n",
    "        tf_idf_dic[elem] = term_frequency[i][elem] * inverse_document_frequency[elem]\n",
    "    \n",
    "    tf_idf.append(tf_idf_dic)\n",
    "\n",
    "print(tf_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
